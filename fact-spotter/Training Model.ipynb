{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facts Claim Spotter",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -O data.zip https://zenodo.org/record/3836810/files/ClaimBuster_Datasets.zip?download=1\n",
        "!unzip data.zip\n",
        "!mv ClaimBuster_Datasets dataset"
      ],
      "metadata": {
        "id": "wcCt-ZkMsbb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install wandb\n",
        "!pip install huggingface_hub\n",
        "!pip install html2text"
      ],
      "metadata": {
        "id": "uYvs9VHqpTDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "wiXWV30p1cqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "OKYgziFNlNbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "%env WANDB_PROJECT=claim_spotter"
      ],
      "metadata": {
        "id": "z3UUJ-2JwLjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "xgtSWZ6Ypotm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "with open('./dataset/datasets/2xNCS.json', 'r') as f:\n",
        "  data =json.loads(f.read())\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.rename(columns={'label':'labels'}, inplace=True)\n",
        "df = df.drop(columns=['sentence_id'])\n",
        "# df['labels'] = df['labels'].map({1:'claim', 0:'not_claim'})\n",
        "df.to_csv('dataset.csv', index=False)\n",
        "\n",
        "# Build random split of the df\n",
        "def build_split(df, train_size=0.8, seed=42):\n",
        "    # Shuffle the dataframe\n",
        "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "    # Split the dataframe\n",
        "    train_size = int(train_size * len(df))\n",
        "    train = df[:train_size]\n",
        "    test = df[train_size:]\n",
        "\n",
        "    return train, test\n",
        "\n",
        "train, test = build_split(df)   \n",
        "train.to_csv('train.csv', index=False)\n",
        "test.to_csv('test.csv', index=False)"
      ],
      "metadata": {
        "id": "6D9JMr1-q9-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('csv', data_files={'train':'train.csv', 'test': 'test.csv'})\n",
        "dataset"
      ],
      "metadata": {
        "id": "pRwrFuwlt2DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "SJxyOYEDp7-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)"
      ],
      "metadata": {
        "id": "L0fo6TkEqCTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "model.config.id2label = {\n",
        "    1: 'claim',\n",
        "    0: 'not_claim'\n",
        "}\n",
        "print('Loaded model')"
      ],
      "metadata": {
        "id": "9IkENFsVqEoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "metric = load_metric(\"f1\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"claim-spotter-multilingual\", \n",
        "    evaluation_strategy=\"epoch\", \n",
        "    report_to=\"wandb\", \n",
        "    push_to_hub=True,\n",
        "    num_train_epochs=2,\n",
        ")"
      ],
      "metadata": {
        "id": "Uw9ub6rWqOhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "GrSQD1OTqar0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "379GEutaqqfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "qtNho8mDvxQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "Sc-_Um_Z1zw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "trained_model = 'gzomer/claim-spotter-multilingual'\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(trained_model, num_labels=2)"
      ],
      "metadata": {
        "id": "EmWqs2c2zfwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim = 'Ukraine is not a country'"
      ],
      "metadata": {
        "id": "drhEdFY82d9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(text, device='cuda'):\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**inputs)\n",
        "    probs = outputs[0].softmax(1)\n",
        "    label = int(probs.argmax().cpu().numpy())\n",
        "    return model.config.id2label[label]\n",
        "\n",
        "def get_predictions(texts, device='cuda'): \n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**inputs)\n",
        "    probs = outputs[0].softmax(1)\n",
        "    labels = [model.config.id2label[item] for item in probs.argmax(1).cpu().numpy()]\n",
        "    return list(zip(texts, labels))"
      ],
      "metadata": {
        "id": "Fw_493Ol2rF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize"
      ],
      "metadata": {
        "id": "HcjE4oAv_Y_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1pcd5KJOAklC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from html2text import HTML2Text\n",
        "\n",
        "def get_page_text(url):\n",
        "  html = requests.get(url).text\n",
        "  soup = BeautifulSoup(html, 'lxml')\n",
        "  soup = soup.find('article')\n",
        "  text = HTML2Text().handle(str(soup))\n",
        "  return text"
      ],
      "metadata": {
        "id": "i0BpeK_-KMyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_entities(text):\n",
        "  return any(c.isupper() for c in text[1:])\n",
        "\n",
        "def filter_sentence(sent):\n",
        "  if '\\n\\n' in sent:\n",
        "    return False\n",
        "  if '[' in sent or ']' in sent:\n",
        "    return False\n",
        "  if '(' in sent or ')' in sent:\n",
        "    return False\n",
        "  if not has_entities(sent):\n",
        "    return False\n",
        "  if len(sent) < 20 or len(sent) > 150:\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "def improve_sentences(sent):\n",
        "  return sent.replace('\\n', ' ')\n",
        "\n",
        "def filter_sentences(sents):\n",
        "  filtered_sents = [sent for sent in sents if filter_sentence(sent)]\n",
        "  filtered_sents = list(set(filtered_sents))\n",
        "  filtered_sents = [improve_sentences(sent) for sent in filtered_sents]\n",
        "  return filtered_sents\n",
        "\n",
        "def find_claims(url):  \n",
        "  text = get_page_text(url)\n",
        "  sents = sent_tokenize(text)\n",
        "  filtered_sents = filter_sentences(sents)  \n",
        "  preds = get_predictions(filtered_sents)\n",
        "  return [item[0].strip() for item in preds if item[1] == 'claim'] \n",
        "\n",
        "url = 'https://www.bbc.com/news/uk-politics-61083402'\n",
        "claims = find_claims(url)  "
      ],
      "metadata": {
        "id": "fmh5bhO3AjL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claims"
      ],
      "metadata": {
        "id": "t13DWBvYJJMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b0OBJDUvKrjw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}