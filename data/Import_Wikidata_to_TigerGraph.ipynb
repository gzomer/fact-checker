{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiCVYGxGMjSw"
      },
      "source": [
        "# Create graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "!pip install -U pyTigerGraph\n",
        "!pip install -U pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpOc1Af_DRz2"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pyTigerGraph as tg\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Connection parameters\n",
        "hostName = \"https://\" # REPLACE WITH YOUR HOSTNAME\n",
        "userName = \"\" # REPLACE WITH YOUR USERNAME\n",
        "password = \"\" # REPLACE WITH YOUR PASSWORD \n",
        "conn = tg.TigerGraphConnection(host=hostName, username=userName, password=password)\n",
        "\n",
        "print(\"Connected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ67pfvRCS9R"
      },
      "outputs": [],
      "source": [
        "def erase():\n",
        "  conn.gsql('''\n",
        "  USE GRAPH Wikidata\n",
        "  DELETE FROM Statement\n",
        "  DELETE FROM subject\n",
        "  DELETE FROM object\n",
        "  DELETE FROM predicate\n",
        "  ''')\n",
        "\n",
        "  results = conn.gsql('''\n",
        "  USE GLOBAL\n",
        "  DROP GRAPH Wikidata\n",
        "  DROP EDGE predicate\n",
        "  DROP VERTEX Object\n",
        "  DROP VERTEX Entity\n",
        "  DROP VERTEX Relation\n",
        "  DROP VERTEX Subject\n",
        "  DROP VERTEX Resource\n",
        "  DROP VERTEX Statement\n",
        "  DROP VERTEX Predicate\n",
        "  DROP EDGE subject\n",
        "  DROP EDGE object\n",
        "  ''')\n",
        "\n",
        "def create():\n",
        "  schema = '''\n",
        "    USE GLOBAL\n",
        "    CREATE VERTEX Resource(PRIMARY_ID id UINT, Name STRING) WITH primary_id_as_attribute=\"true\"\n",
        "    CREATE VERTEX Statement(PRIMARY_ID id UINT, Name STRING) WITH primary_id_as_attribute=\"true\"\n",
        "    CREATE VERTEX Predicate(PRIMARY_ID id UINT, Name STRING) WITH primary_id_as_attribute=\"true\"\n",
        "    CREATE DIRECTED EDGE object (FROM Statement, TO Resource)\n",
        "    CREATE DIRECTED EDGE subject (FROM Statement, TO Resource)\n",
        "    CREATE DIRECTED EDGE predicate (FROM Statement, TO Predicate)\n",
        "  '''\n",
        "  conn.gsql(schema)\n",
        "  results = conn.gsql('CREATE GRAPH Wikidata(Resource, Statement, Predicate, object, subject, predicate)')\n",
        "\n",
        "  results = conn.gsql('''\n",
        "    USE GRAPH Wikidata\n",
        "    BEGIN\n",
        "    CREATE LOADING JOB load_predicate FOR GRAPH Wikidata {\n",
        "    DEFINE FILENAME MyDataSource;  \n",
        "    LOAD MyDataSource TO VERTEX Predicate VALUES($0, $1) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
        "    }\n",
        "    END\n",
        "    ''')\n",
        "  print(results)\n",
        "\n",
        "  results = conn.gsql('''\n",
        "    USE GRAPH Wikidata\n",
        "    BEGIN\n",
        "    CREATE LOADING JOB load_entities FOR GRAPH Wikidata {\n",
        "    DEFINE FILENAME MyDataSource;  \n",
        "    LOAD MyDataSource TO VERTEX Resource VALUES($0, $1) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
        "    }\n",
        "    END\n",
        "    ''')\n",
        "  print(results)\n",
        "\n",
        "  results = conn.gsql('''\n",
        "    USE GRAPH Wikidata\n",
        "    BEGIN\n",
        "    CREATE LOADING JOB load_statements FOR GRAPH Wikidata {\n",
        "    DEFINE FILENAME MyDataSource;  \n",
        "    LOAD MyDataSource TO VERTEX Statement VALUES($0, \"\") USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
        "    LOAD MyDataSource TO EDGE subject VALUES($0, $1) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
        "    LOAD MyDataSource TO EDGE predicate VALUES($0, $2) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
        "    LOAD MyDataSource TO EDGE object VALUES($0, $3) USING SEPARATOR=\",\", HEADER=\"true\", EOL=\"\\\\n\", QUOTE=\"double\";\n",
        "    }\n",
        "    END\n",
        "    ''')\n",
        "  print(results)\n",
        "  \n",
        "create()  \n",
        "\n",
        "conn.graphname=\"Wikidata\"\n",
        "secret = conn.createSecret()\n",
        "authToken = conn.getToken(secret)\n",
        "authToken = authToken[0]\n",
        "\n",
        "conn = tg.TigerGraphConnection(host=hostName, graphname=\"Wikidata\", username=userName, password=password, apiToken=authToken)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAl24xkADJ9n"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsK-QxW7D49V"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/lnbhc8yuhit4wm5/wikidata5m_alias.tar.gz?dl=1 -O wikidata5m_alias.tar.gz\n",
        "!wget https://www.dropbox.com/s/563omb11cxaqr83/wikidata5m_all_triplet.txt.gz?dl=1 -O wikidata5m_all_triplet.txt.gz\n",
        "!gunzip ./wikidata5m_all_triplet.txt.gz\n",
        "!tar -zxvf wikidata5m_alias.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6mLbDn7D8k7"
      },
      "outputs": [],
      "source": [
        "with open('./wikidata5m_relation.txt') as f:\n",
        "  relations = [item.strip().split('\\t') for item in f.readlines()]\n",
        "\n",
        "with open('./wikidata5m_entity.txt') as f:\n",
        "  entities = [item.strip().split('\\t') for item in f.readlines()]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FafCFxhEDjxb"
      },
      "outputs": [],
      "source": [
        "def export_data(data, filename, per_page=None):\n",
        "  rows = []\n",
        "  for item in data:\n",
        "    rows.append({\n",
        "        'id': int(item[0][1:]),\n",
        "        'value': item[1],\n",
        "    })\n",
        "  if per_page is None:\n",
        "    pd.DataFrame(rows).to_csv(f'./{filename}.csv', index=False)\n",
        "  else:\n",
        "    # Split rows in pages of size per_page\n",
        "    pages = [rows[i:i+per_page] for i in range(0, len(rows), per_page)]\n",
        "    for i, page in enumerate(pages):\n",
        "        pd.DataFrame(page).to_csv(f'./{filename}_{i}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDVTnLdYDp5v"
      },
      "source": [
        "## Loading predicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQXJVP0BTk__"
      },
      "outputs": [],
      "source": [
        "export_data(relations, 'relations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwLLN1TqTYlP"
      },
      "outputs": [],
      "source": [
        "# Load the posts file wiht the 'load_posts' job\n",
        "posts_file = './relations.csv'\n",
        "results = conn.uploadFile(posts_file, fileTag='MyDataSource', jobName='load_predicate')\n",
        "print(json.dumps(results, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HB_uF9_D6gD"
      },
      "source": [
        "## Loading entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5x3s9jLY62g"
      },
      "outputs": [],
      "source": [
        "os.makedirs('entities', exist_ok=True)\n",
        "export_data(entities, 'entities/entities', per_page=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nee7YhvwY8Wt"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "import tqdm\n",
        "import time\n",
        "def upload_data(folder, job, wait=2, start_from=0):\n",
        "    files = listdir(folder)\n",
        "    # Filter csv\n",
        "    csv_files = [f for f in files if f.endswith('.csv')]\n",
        "    # Upload\n",
        "    count = 0\n",
        "    for file in tqdm.tqdm(csv_files):   \n",
        "        if count < start_from: \n",
        "          count+=1    \n",
        "          continue\n",
        "        posts_file = f'{folder}/{file}'\n",
        "        results = conn.uploadFile(posts_file, fileTag='MyDataSource', jobName=job)\n",
        "        #print('File', file)\n",
        "        time.sleep(wait)\n",
        "        count+=1\n",
        "\n",
        "upload_data('./entities', 'load_entities')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dxiMg1RD9bz"
      },
      "source": [
        "## Loading facts triplets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtcqMNHwe_o2",
        "outputId": "08f85713-f474-435c-ef44-1b35137aa55e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21354359/21354359 [00:48<00:00, 438841.63it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tqdm\n",
        "import os\n",
        "\n",
        "def save_triplets(folder, per_page=5000):    \n",
        "\n",
        "    num_rows = os.popen('wc -l wikidata5m_all_triplet.txt').read().split()[0]    \n",
        "    current_file_index = 0\n",
        "    triplets_file = open(f'{folder}/triplet{current_file_index}.csv', 'a')\n",
        "    triplets_file.write('statement,s,v,o\\n')\n",
        "    current_count = 0\n",
        "    with open('./wikidata5m_all_triplet.txt') as f:\n",
        "        for index, row in tqdm.tqdm(enumerate(f), total=int(num_rows)):\n",
        "            if current_count == per_page:\n",
        "                current_count = 0\n",
        "                current_file_index += 1\n",
        "                triplets_file.close()\n",
        "                triplets_file = open(f'{folder}/triplet{current_file_index}.csv', 'a')\n",
        "                triplets_file.write('statement,s,v,o\\n')\n",
        "                \n",
        "            s, v, o = row.strip().split('\\t')\n",
        "            data = [\n",
        "                str(index+1), s[1:],v[1:],o[1:]\n",
        "            ]\n",
        "            triplets_file.write(','.join(data)+'\\n')\n",
        "            current_count+=1\n",
        "\n",
        "    # Add last batch\n",
        "    triplets_file.close()\n",
        "\n",
        "os.makedirs('triplets', exist_ok=True)\n",
        "save_triplets('triplets', per_page=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9QSe-YHU0BS"
      },
      "outputs": [],
      "source": [
        "upload_data('./triplets', 'load_statements', start_from=0, wait=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Import Wikidata to TigerGraph",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
